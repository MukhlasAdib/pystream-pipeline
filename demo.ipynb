{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyStream Pipeline Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some examples of how to use PyStream to construct real time data pipeline. For details, please also visit the package [documentation](https://pystream-pipeline.readthedocs.io/). First, let's import related packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "from pystream import Pipeline, Stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Stages"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyStream, a pipeline is created based on several components, which is called `Stages`. The data given to the pipeline will be passed from one stage to another until they reach the final stage.\n",
    "\n",
    "Stage can be defined in two ways:\n",
    "- As a custom class instance. The class must be inherited from ``pystream.Stage`` abstract class and has ``__call__`` and ``cleanup`` methods defined.\n",
    "- As a function. The function must only takes one argument and return one result of the same type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a sample stage. A stage below perform some convolutions on an image (3D numpy array with shape HxWx3) contained in a dictionary. The result is then put in the same dictionary as the input, replacing the input image. The class also keeps track of the number of executions and reset the counter to 0 during cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyStage(Stage):\n",
    "    \"\"\"A dummy stage that performs some convolutions to the input 2D array,\n",
    "    and count how many input it has processed.\n",
    "    \n",
    "    For stages in class form, the __call__ and cleanup methods have to be \n",
    "    defined, and it is recommended to define 'name' property in the stage \n",
    "    instance init\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        self.count = 0\n",
    "        self.name = name\n",
    "        self.kernel = np.random.randint(-10, 10, size=(5, 5))\n",
    "\n",
    "    def __call__(self, data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        img = data[\"data\"]\n",
    "        for _ in range(100):\n",
    "            img = cv2.filter2D(src=img, ddepth=-1, kernel=self.kernel)\n",
    "        data[\"data\"] = img\n",
    "        self.count += 1\n",
    "        return data\n",
    "\n",
    "    def cleanup(self):\n",
    "        self.count = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also define the above stage as a function without the counter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy_stage_func(data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "    \"\"\"A function that does some convolutions to the input.\n",
    "    \n",
    "    This is a dummy stage defined as a function. A stage in function form only \n",
    "    takes one argument and return one result with same type\"\"\"\n",
    "\n",
    "    img = data[\"data\"]\n",
    "    kernel = np.random.randint(-10, 10, size=(5, 5))\n",
    "    for _ in range(100):\n",
    "        img = cv2.filter2D(src=img, ddepth=-1, kernel=kernel)\n",
    "    data[\"data\"] = img\n",
    "    return data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run pipeline autonomously, i.e., the data will be generated automatically at each specific seconds. To do that, you need to define a data generator, which is a callable that takes no argument and return the data. In this example, we only want to generate a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data() -> Dict[str, np.ndarray]:\n",
    "    img = np.random.randint(0, 255, size=(480, 720, 3), dtype=np.uint8)\n",
    "    return {\"data\": img}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle period for the input data\n",
    "INPUT_PERIOD = 0.2\n",
    "# Time to run the pipeline\n",
    "ON_TIME = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline. You only need to use `pystream.Pipeline` to create it. Here I put the pipeline creation into `create_pipeline` function. I also made a helper function `print_profile` to print profiling results later. In this particular example we use only the class-based stage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stages() -> List[DummyStage]:\n",
    "    # Return 5 dummy stages\n",
    "    return [DummyStage(f\"Stage{i + 1}\") for i in range(5)]\n",
    "\n",
    "def create_pipeline() -> Tuple[Pipeline, List[DummyStage]]:\n",
    "    # First, create the pipeline instance, we want to use the profiler\n",
    "    pipeline = Pipeline(input_generator=generate_data, use_profiler=True)\n",
    "    # Create the stages\n",
    "    stages = create_stages()\n",
    "    # Now, add the stages to the pipeline.\n",
    "    for stage in stages:\n",
    "        pipeline.add(stage)\n",
    "    return pipeline, stages\n",
    "\n",
    "def print_profile(latency: Dict[str, float], throughput: Dict[str, float]) -> None:\n",
    "    data = []\n",
    "    for k in latency.keys():\n",
    "        d = [k, latency[k], throughput[k]]\n",
    "        data.append(d)\n",
    "    table = tabulate(data, headers=[\"Stage\", \"Latency (s)\", \"Throughput (d/s)\"])\n",
    "    print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready! Run the pipeline in serial mode. Use `get_profiles` method of `pystream.Pipeline` to get the latency and throughput records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline in serial...\n",
      "Streaming data each 0.2 s...\n",
      "Waiting for 5 s...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping pipeline...\n",
      "\n",
      "Last output shape:\n",
      "(480, 720, 3)\n",
      "Number of processed data:\n",
      "6\n",
      "Pipeline has been cleaned-up\n",
      "Data counter was reset to 0.\n",
      "\n",
      "Pipeline profile:\n",
      "Stage                   Latency (s)    Throughput (d/s)\n",
      "--------------------  -------------  ------------------\n",
      "MainPipeline               0.885433             1.10035\n",
      "MainPipeline__Stage1       0.168262             1.09003\n",
      "MainPipeline__Stage2       0.170458             1.09568\n",
      "MainPipeline__Stage3       0.182034             1.09805\n",
      "MainPipeline__Stage4       0.182642             1.09975\n",
      "MainPipeline__Stage5       0.181923             1.10035\n"
     ]
    }
   ],
   "source": [
    "pipeline, stages = create_pipeline()\n",
    "print(\"Starting pipeline in serial...\")\n",
    "pipeline.serialize()\n",
    "print(f\"Streaming data each {INPUT_PERIOD} s...\")\n",
    "pipeline.start_loop(INPUT_PERIOD)\n",
    "print(f\"Waiting for {ON_TIME} s...\")\n",
    "time.sleep(ON_TIME)\n",
    "print(\"Stopping pipeline...\")\n",
    "pipeline.stop_loop()\n",
    "\n",
    "# Let's try read the last result and do cleanup\n",
    "latest = pipeline.get_results()\n",
    "print()\n",
    "print(\"Last output shape:\")\n",
    "print(latest[\"data\"].shape)\n",
    "print(\"Number of processed data:\")\n",
    "print(stages[-1].count)\n",
    "pipeline.cleanup()\n",
    "print(\"Pipeline has been cleaned-up\")\n",
    "print(f\"Data counter was reset to {stages[-1].count}.\")\n",
    "\n",
    "# Get the profile\n",
    "latency, throughput = pipeline.get_profiles()\n",
    "print()\n",
    "print(\"Pipeline profile:\")\n",
    "print_profile(latency, throughput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing, but now use parallel pipeline instead of serial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline in parallel...\n",
      "Streaming data each 0.2 s...\n",
      "Waiting for 5 s...\n",
      "Stopping pipeline...\n",
      "\n",
      "Last output shape:\n",
      "(480, 720, 3)\n",
      "Number of processed data:\n",
      "19\n",
      "Pipeline has been cleaned-up\n",
      "Data counter was reset to 0.\n",
      "\n",
      "Pipeline profile:\n",
      "Stage                   Latency (s)    Throughput (d/s)\n",
      "--------------------  -------------  ------------------\n",
      "MainPipeline               1.15793              4.48034\n",
      "MainPipeline__Stage1       0.215918             4.58622\n",
      "MainPipeline__Stage2       0.218174             4.53008\n",
      "MainPipeline__Stage3       0.204841             4.50127\n",
      "MainPipeline__Stage4       0.20452              4.47497\n",
      "MainPipeline__Stage5       0.210609             4.48047\n"
     ]
    }
   ],
   "source": [
    "pipeline, stages = create_pipeline()\n",
    "print(\"Starting pipeline in parallel...\")\n",
    "pipeline.parallelize()\n",
    "print(f\"Streaming data each {INPUT_PERIOD} s...\")\n",
    "pipeline.start_loop(INPUT_PERIOD)\n",
    "print(f\"Waiting for {ON_TIME} s...\")\n",
    "time.sleep(ON_TIME)\n",
    "print(\"Stopping pipeline...\")\n",
    "pipeline.stop_loop()\n",
    "\n",
    "# Let's try read the last result and do cleanup\n",
    "latest = pipeline.get_results()\n",
    "print()\n",
    "print(\"Last output shape:\")\n",
    "print(latest[\"data\"].shape)\n",
    "print(\"Number of processed data:\")\n",
    "print(stages[-1].count)\n",
    "pipeline.cleanup()\n",
    "print(\"Pipeline has been cleaned-up\")\n",
    "print(f\"Data counter was reset to {stages[-1].count}.\")\n",
    "\n",
    "# Get the profile\n",
    "latency, throughput = pipeline.get_profiles()\n",
    "print()\n",
    "print(\"Pipeline profile:\")\n",
    "print_profile(latency, throughput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the above profile with the previous one. You might notice several things:\n",
    "\n",
    "- The throughput has been significantly increased, thanks to the parallelization of the pipeline\n",
    "- Thus, the number of processed data is increased\n",
    "- You might get the latency slower than the serial, which is due to the resource utilization of your CPU. In this experiment, we run all of the stages at the same time."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Manually Triggered Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can also pass the data to the pipeline manually. Just use the `forward` method. Let's recreate the pipeline. This time, we use both class and function-based stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_pipeline() -> Pipeline:\n",
    "    # First, create the pipeline instance, we want to use the profiler\n",
    "    pipeline = Pipeline(input_generator=generate_data, use_profiler=True)\n",
    "    # Now, add the stages to the pipeline.\n",
    "    pipeline.add(DummyStage(\"StageA\"))\n",
    "    pipeline.add(DummyStage(\"StageB\"))\n",
    "    pipeline.add(dummy_stage_func)\n",
    "    pipeline.add(dummy_stage_func)\n",
    "    return pipeline\n",
    "\n",
    "def print_profile(latency: Dict[str, float], throughput: Dict[str, float]) -> None:\n",
    "    data = []\n",
    "    for k in latency.keys():\n",
    "        d = [k, latency[k], throughput[k]]\n",
    "        data.append(d)\n",
    "    table = tabulate(data, headers=[\"Stage\", \"Latency (s)\", \"Throughput (d/s)\"])\n",
    "    print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Times to run the pipeline\n",
    "ON_CYCLE = 10"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try to pass the data manually by using the `forward` method. Let's just use parallel pipeline this time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline in parallel...\n",
      "\n",
      "Last output shape:\n",
      "(480, 720, 3)\n",
      "Pipeline has been cleaned-up\n",
      "\n",
      "Pipeline profile:\n",
      "Stage                    Latency (s)    Throughput (d/s)\n",
      "---------------------  -------------  ------------------\n",
      "MainPipeline                1.15573              5.02434\n",
      "MainPipeline__StageA        0.203491             4.92226\n",
      "MainPipeline__StageB        0.188007             4.90051\n",
      "MainPipeline__Stage_1       0.190019             4.91418\n",
      "MainPipeline__Stage_2       0.188901             5.02444\n"
     ]
    }
   ],
   "source": [
    "pipeline = create_pipeline()\n",
    "print(\"Starting pipeline in parallel...\")\n",
    "# Wait for 5 seconds if the pipeline is not ready to take data yet when\n",
    "# pushing data to the pipeline\n",
    "pipeline.parallelize(block_input=True, input_timeout=10)\n",
    "\n",
    "# Generate and forward the data for ON_CYCLE times\n",
    "for _ in range(ON_CYCLE):\n",
    "    data = {\n",
    "        \"data\": np.random.randint(0, 255, size=(480, 720, 3), dtype=np.uint8)\n",
    "    }\n",
    "    pipeline.forward(data)\n",
    "time.sleep(5)\n",
    "\n",
    "# Let's try read the last result and do cleanup\n",
    "latest = pipeline.get_results()\n",
    "print()\n",
    "print(\"Last output shape:\")\n",
    "print(latest[\"data\"].shape)\n",
    "pipeline.cleanup()\n",
    "print(\"Pipeline has been cleaned-up\")\n",
    "\n",
    "# Get the profile\n",
    "latency, throughput = pipeline.get_profiles()\n",
    "print()\n",
    "print(\"Pipeline profile:\")\n",
    "print_profile(latency, throughput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mixed Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to combine serial and parallel pipeline, i.e., embed a pipeline in another pipeline. To do that, you need to convert the child pipeline into a compatible form. It is easy, you only need to invoke `as_stage` method of `Pipeline`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this experiment, we will embed a serial pipeline into a parallel pipeline. First, let's define the child pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pystream.pipeline.pipeline.Pipeline at 0x216908f3700>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_child_pipeline() -> Pipeline:\n",
    "    # First, create the pipeline instance, we want to use the profiler\n",
    "    pipeline = Pipeline(input_generator=generate_data, use_profiler=True)\n",
    "    # Now, add the stages to the pipeline.\n",
    "    pipeline.add(DummyStage(\"StageA\"))\n",
    "    pipeline.add(DummyStage(\"StageB\"))\n",
    "    return pipeline\n",
    "\n",
    "child_pipeline = create_child_pipeline()\n",
    "child_pipeline.serialize()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, create a parent pipeline with the child pipeline in it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<pystream.pipeline.pipeline.Pipeline at 0x216908f3dc0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_parent_pipeline(child_pipeline: Pipeline) -> Pipeline:\n",
    "    # First, create the pipeline instance, we want to use the profiler\n",
    "    pipeline = Pipeline(input_generator=generate_data, use_profiler=True)\n",
    "    # Now, add the stages to the pipeline. The last stage is the child pipeline.\n",
    "    pipeline.add(DummyStage(\"Stage1\"))\n",
    "    pipeline.add(DummyStage(\"Stage2\"))\n",
    "    pipeline.add(child_pipeline.as_stage(), \"ChildPipeline\")\n",
    "    return pipeline\n",
    "\n",
    "parent_pipeline = create_parent_pipeline(child_pipeline)\n",
    "parent_pipeline.parallelize(block_input=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try it out!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Last output shape:\n",
      "(480, 720, 3)\n",
      "Pipeline has been cleaned-up\n",
      "\n",
      "Pipeline profile:\n",
      "Stage                                  Latency (s)    Throughput (d/s)\n",
      "-----------------------------------  -------------  ------------------\n",
      "MainPipeline                              1.97543              2.75196\n",
      "MainPipeline__Stage1                      0.191864             4.40831\n",
      "MainPipeline__Stage2                      0.191059             3.57146\n",
      "MainPipeline__ChildPipeline               0.363558             2.75198\n",
      "MainPipeline__ChildPipeline__StageA       0.187975             2.74605\n",
      "MainPipeline__ChildPipeline__StageB       0.175553             2.75198\n"
     ]
    }
   ],
   "source": [
    "# Generate and forward the data for ON_CYCLE times\n",
    "ON_CYCLE = 10\n",
    "for _ in range(ON_CYCLE):\n",
    "    data = {\n",
    "        \"data\": np.random.randint(0, 255, size=(480, 720, 3), dtype=np.uint8)\n",
    "    }\n",
    "    parent_pipeline.forward(data)\n",
    "time.sleep(5)\n",
    "\n",
    "# Let's try read the last result and do cleanup\n",
    "latest = parent_pipeline.get_results()\n",
    "print()\n",
    "print(\"Last output shape:\")\n",
    "print(latest[\"data\"].shape)\n",
    "parent_pipeline.cleanup()\n",
    "print(\"Pipeline has been cleaned-up\")\n",
    "\n",
    "# Get the profile\n",
    "latency, throughput = parent_pipeline.get_profiles()\n",
    "print()\n",
    "print(\"Pipeline profile:\")\n",
    "print_profile(latency, throughput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might have noticed that in the profiling reports, the pipeline levels are separated with `__`."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystream-pipeline-299M7DxG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6467d7306d71ffee6e04c2423aa30d05f8dd8bb79f51d418b1e0875e674c3626"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
