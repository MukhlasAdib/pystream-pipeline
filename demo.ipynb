{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PyStream Pipeline Demo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains some examples of how to use PyStream to construct real time data pipeline. For details, please also visit the package [documentation](https://pystream-pipeline.readthedocs.io/). First, let's import related packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "\n",
    "from pystream import Pipeline, Stage"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stage Definitions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In PyStream, a pipeline is created based on several components, which is called `Stages`. The data given to the pipeline will be passed from one stage to another until they reach the final stage.\n",
    "\n",
    "Stage can be defined in two ways:\n",
    "- As a custom class instance. The class must be inherited from ``pystream.Stage`` abstract class and has ``__call__`` and ``cleanup`` methods defined.\n",
    "- As a function. The function must only takes one argument and return one result of the same type."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's create a sample stage. A stage below perform some convolutions on an image (3D numpy array with shape HxWx3) contained in a dictionary. The result is then put in the same dictionary as the input, replacing the input image. The class also keeps track of the number of executions and reset the counter to 0 during cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DummyStage(Stage):\n",
    "    \"\"\"A dummy stage that performs some convolutions to the input 2D array,\n",
    "    and count how many input it has processed.\n",
    "    \n",
    "    It is recommended to define 'name' property in the stage instance init\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, name: str) -> None:\n",
    "        \"\"\"Initialize stage\n",
    "\n",
    "        Args:\n",
    "            name (str): stage name\n",
    "        \"\"\"        \n",
    "        self.count = 0\n",
    "        self.name = name\n",
    "        self.kernel = np.random.randint(-10, 10, size=(5, 5))\n",
    "\n",
    "    def __call__(self, data: Dict[str, np.ndarray]) -> Dict[str, np.ndarray]:\n",
    "        \"\"\"Main data processing of the stage, a mandatory method of the Stage \n",
    "        class. Note that the stage does not return a new dict object, but only\n",
    "        modifies the input object and returns it as the output\n",
    "\n",
    "        Args:\n",
    "            data (Dict[str, np.ndarray]): the input array, packed\n",
    "                in a dictionary\n",
    "\n",
    "        Returns:\n",
    "            Dict[str, np.ndarray]: the output array, packed\n",
    "            in a dictionary\n",
    "        \"\"\"\n",
    "        img = data[\"data\"]\n",
    "        for _ in range(100):\n",
    "            img = cv2.filter2D(src=img, ddepth=-1, kernel=self.kernel)\n",
    "        data[\"data\"] = img\n",
    "        self.count += 1\n",
    "        return data\n",
    "\n",
    "    def cleanup(self):\n",
    "        \"\"\"Cleanup method, called at the end of the pipeline.\n",
    "        in this example, we want to reset the counter to 0\"\"\"\n",
    "        self.count = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Automated Pipeline"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also run pipeline autonomously, i.e., the data will be generated automatically at each specific seconds. To do that, you need to define a data generator, which is a callable that takes no argument and return the data. In this example, we only want to generate a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data() -> Dict[str, np.ndarray]:\n",
    "    img = np.random.randint(0, 255, size=(480, 720, 3), dtype=np.uint8)\n",
    "    return {\"data\": img}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, let's define some parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cycle period for the input data\n",
    "INPUT_PERIOD = 0.2\n",
    "# Time to run the pipeline\n",
    "ON_TIME = 5"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create the pipeline. You only need to use `pystream.Pipeline` to create it. Here I put the pipeline creation into `create_pipeline` function. I also made a helper function `print_profile` to print profiling results later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_stages() -> List[DummyStage]:\n",
    "    # Return 5 dummy stages\n",
    "    return [DummyStage(f\"Stage{i + 1}\") for i in range(5)]\n",
    "\n",
    "def create_pipeline() -> Tuple[Pipeline, List[DummyStage]]:\n",
    "    # First, create the pipeline instance, we want to use the profiler\n",
    "    pipeline = Pipeline(input_generator=generate_data, use_profiler=True)\n",
    "    # Create the stages\n",
    "    stages = create_stages()\n",
    "    # Now, add the stages to the pipeline.\n",
    "    for stage in stages:\n",
    "        pipeline.add(stage)\n",
    "    return pipeline, stages\n",
    "\n",
    "def print_profile(latency: Dict[str, float], throughput: Dict[str, float]) -> None:\n",
    "    data = []\n",
    "    for k in latency.keys():\n",
    "        d = [k, latency[k], throughput[k]]\n",
    "        data.append(d)\n",
    "    table = tabulate(data, headers=[\"Stage\", \"Latency (s)\", \"Throughput (d/s)\"])\n",
    "    print(table)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are ready! Run the pipeline in serial mode. Use `get_profiles` method of `pystream.Pipeline` to get the latency and throughput records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline in serial...\n",
      "Streaming data each 0.2 s...\n",
      "Waiting for 5 s...\n",
      "Stopping pipeline...\n",
      "\n",
      "Last output shape:\n",
      "(480, 720, 3)\n",
      "Number of processed data:\n",
      "6\n",
      "Pipeline has been cleaned-up\n",
      "Data counter was reset to 0.\n",
      "\n",
      "Pipeline profile:\n",
      "Stage       Latency (s)    Throughput (d/s)\n",
      "--------  -------------  ------------------\n",
      "Stage1         0.174785             1.07098\n",
      "Stage2         0.192589             1.07223\n",
      "Stage3         0.189225             1.07225\n",
      "Stage4         0.172093             1.06987\n",
      "Stage5         0.182117             1.06914\n",
      "Pipeline       0.910843             1.06914\n"
     ]
    }
   ],
   "source": [
    "pipeline, stages = create_pipeline()\n",
    "print(\"Starting pipeline in serial...\")\n",
    "pipeline.serialize()\n",
    "print(f\"Streaming data each {INPUT_PERIOD} s...\")\n",
    "pipeline.start_loop(INPUT_PERIOD)\n",
    "print(f\"Waiting for {ON_TIME} s...\")\n",
    "time.sleep(ON_TIME)\n",
    "print(\"Stopping pipeline...\")\n",
    "pipeline.stop_loop()\n",
    "\n",
    "# Let's try read the last result and do cleanup\n",
    "latest = pipeline.get_results()\n",
    "print()\n",
    "print(\"Last output shape:\")\n",
    "print(latest[\"data\"].shape)\n",
    "print(\"Number of processed data:\")\n",
    "print(stages[-1].count)\n",
    "pipeline.cleanup()\n",
    "print(\"Pipeline has been cleaned-up\")\n",
    "print(f\"Data counter was reset to {stages[-1].count}.\")\n",
    "\n",
    "# Get the profile\n",
    "latency, throughput = pipeline.get_profiles()\n",
    "print()\n",
    "print(\"Pipeline profile:\")\n",
    "print_profile(latency, throughput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's do the same thing, but now use parallel pipeline instead of serial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting pipeline in serial...\n",
      "Streaming data each 0.2 s...\n",
      "Waiting for 5 s...\n",
      "Stopping pipeline...\n",
      "\n",
      "Last output shape:\n",
      "(480, 720, 3)\n",
      "Number of processed data:\n",
      "17\n",
      "Pipeline has been cleaned-up\n",
      "Data counter was reset to 0.\n",
      "\n",
      "Pipeline profile:\n",
      "Stage       Latency (s)    Throughput (d/s)\n",
      "--------  -------------  ------------------\n",
      "Stage1         0.230675             4.36388\n",
      "Stage2         0.21523              4.33035\n",
      "Stage3         0.244283             4.11553\n",
      "Stage4         0.244383             4.08065\n",
      "Stage5         0.21612              4.08475\n",
      "Pipeline       1.44614              4.08461\n"
     ]
    }
   ],
   "source": [
    "pipeline, stages = create_pipeline()\n",
    "print(\"Starting pipeline in serial...\")\n",
    "pipeline.parallelize()\n",
    "print(f\"Streaming data each {INPUT_PERIOD} s...\")\n",
    "pipeline.start_loop(INPUT_PERIOD)\n",
    "print(f\"Waiting for {ON_TIME} s...\")\n",
    "time.sleep(ON_TIME)\n",
    "print(\"Stopping pipeline...\")\n",
    "pipeline.stop_loop()\n",
    "\n",
    "# Let's try read the last result and do cleanup\n",
    "latest = pipeline.get_results()\n",
    "print()\n",
    "print(\"Last output shape:\")\n",
    "print(latest[\"data\"].shape)\n",
    "print(\"Number of processed data:\")\n",
    "print(stages[-1].count)\n",
    "pipeline.cleanup()\n",
    "print(\"Pipeline has been cleaned-up\")\n",
    "print(f\"Data counter was reset to {stages[-1].count}.\")\n",
    "\n",
    "# Get the profile\n",
    "latency, throughput = pipeline.get_profiles()\n",
    "print()\n",
    "print(\"Pipeline profile:\")\n",
    "print_profile(latency, throughput)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare the above profile with the previous one. You might notice several things:\n",
    "\n",
    "- The throughput has been significantly increased, thanks to the parallelization of the pipeline\n",
    "- Thus, the number of processed data is increased\n",
    "- You might get the latency slower than the serial, which is due to the resource utilization of your CPU. Now we run all of the stages at the same time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pystream-pipeline-299M7DxG-py3.10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6467d7306d71ffee6e04c2423aa30d05f8dd8bb79f51d418b1e0875e674c3626"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
